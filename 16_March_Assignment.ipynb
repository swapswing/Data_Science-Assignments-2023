{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e11721-f0cf-4839-8ff1-8336c8cdf867",
   "metadata": {},
   "source": [
    "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\n",
    "\n",
    "Ans.\n",
    "\n",
    "Overfitting: When a model learns not only the underlying patterns in the training data but also the noise and details, resulting in poor generalization to new data.\n",
    "\n",
    "Consequences: High accuracy on training data but poor performance on unseen data.\n",
    "Mitigation: Use regularization techniques, reduce model complexity, add more training data, use dropout, and perform cross-validation.\n",
    "Underfitting: When a model is too simple to capture the underlying structure of the data, resulting in poor performance on both training and test data.\n",
    "\n",
    "Consequences: Low accuracy on both training and unseen data.\n",
    "Mitigation: Increase model complexity, use better feature engineering, and train the model for a longer time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0771d38-4150-475c-8323-1b3eac2cf28a",
   "metadata": {},
   "source": [
    "# Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "Ans.\n",
    "\n",
    "Regularization: Add a penalty to the loss function to prevent large weights (e.g., L1 or L2 regularization).\n",
    "\n",
    "Simplify the Model: Reduce the number of parameters or layers in the model.\n",
    "\n",
    "Cross-Validation: Use techniques like k-fold cross-validation to evaluate the model's performance.\n",
    "\n",
    "Add More Data: Increasing the training dataset size can help the model generalize better.\n",
    "\n",
    "Data Augmentation: Generate more variations of the existing data to enhance diversity.\n",
    "\n",
    "Dropout: Randomly drop neurons during training to prevent reliance on specific features.\n",
    "\n",
    "Early Stopping: Stop training when performance on the validation set stops improving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae421f-18d5-4ffe-8b76-ea1371d612d3",
   "metadata": {},
   "source": [
    "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "Ans.\n",
    "\n",
    "Underfitting occurs when a model is too simplistic to capture the complexities in the data.\n",
    "Scenarios:\n",
    "Using a linear model for non-linear data.\n",
    "Insufficient training epochs for complex models.\n",
    "Poor feature selection or insufficient features.\n",
    "High regularization penalties that overly constrain the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be828c7-b454-4445-95d8-4699712ddf60",
   "metadata": {},
   "source": [
    "# Q4: Explain the Bias-Variance Tradeoff in Machine Learning. What Is the Relationship Between Bias and Variance, and How Do They Affect Model Performance?\n",
    "\n",
    "Ans.\n",
    "\n",
    "Bias: Error due to overly simplistic assumptions in the learning algorithm. High bias results in underfitting.\n",
    "\n",
    "Variance: Error due to sensitivity to fluctuations in the training data. High variance results in overfitting.\n",
    "\n",
    "Tradeoff:\n",
    "\n",
    "High bias -> Low variance, poor performance (underfitting).\n",
    "High variance -> Low bias, poor generalization (overfitting).\n",
    "Goal: Minimize both bias and variance to achieve an optimal tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4d4a7-658e-4516-9d98-ebe9b86445a0",
   "metadata": {},
   "source": [
    "# Q5: Methods for Detecting Overfitting and Underfitting in ML Models\n",
    "Detecting Overfitting:\n",
    "\n",
    "High training accuracy and low test/validation accuracy.\n",
    "Large gap between training and test errors.\n",
    "Detecting Underfitting:\n",
    "\n",
    "Low accuracy on both training and test/validation datasets.\n",
    "Training loss does not decrease significantly.\n",
    "Techniques:\n",
    "\n",
    "Plot learning curves (train vs. validation loss).\n",
    "Evaluate metrics on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747e04b-93cd-4379-a644-14700e09d810",
   "metadata": {},
   "source": [
    "# Q6: Compare and Contrast Bias and Variance in Machine Learning\n",
    "Bias:\n",
    "\n",
    "Represents systematic error.\n",
    "High in simple models (e.g., linear regression).\n",
    "Causes underfitting.\n",
    "\n",
    "\n",
    "Variance:\n",
    "\n",
    "Represents sensitivity to training data.\n",
    "High in complex models (e.g., deep neural networks).\n",
    "Causes overfitting.\n",
    "Key Difference: High-bias models fail to learn enough from data, while high-variance models learn too much, including noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f811505-5ba0-4163-92f6-112a2450fd85",
   "metadata": {},
   "source": [
    "# Q7: What Is Regularization in Machine Learning, and How Can It Be Used to Prevent Overfitting?\n",
    "\n",
    "Definition: Regularization is a technique to penalize model complexity and prevent overfitting by modifying the loss function.\n",
    "\n",
    "Common Techniques:\n",
    "\n",
    "L1 Regularization (Lasso): Adds the sum of absolute weights to the loss function, encouraging sparsity.\n",
    "L2 Regularization (Ridge): Adds the sum of squared weights to the loss function, encouraging small weights.\n",
    "Elastic Net: Combines L1 and L2 regularization.\n",
    "Dropout: Randomly disables neurons during training.\n",
    "Weight Decay: Directly reduces weight magnitude during optimization.\n",
    "By controlling model complexity, regularization ensures better generalization to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16684e73-0664-4138-94f7-4f120ee6736e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
